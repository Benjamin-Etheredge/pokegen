{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "d11cadb2604831264ea35f48e116d1fe3e92b6fc7f99ae215998bba77e4a2980"
   }
  },
  "interpreter": {
   "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"../data/external/sprites/items/berries/aguav-berry.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.tensor(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import os\n",
    "from skimage import io, transform\n",
    "\n",
    "\n",
    "#ds = datasets.ImageFolder('../data/external/')\n",
    "class PokemonDataset(Dataset):\n",
    "    \n",
    "    normal_sprites_sub_dir = \"pokemon\"\n",
    "    female_sub_dir = \"female\"\n",
    "\n",
    "    def __init__(self, sprites_path, transform=None):\n",
    "        self.sprites_path = sprites_path\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(os.path.join(sprites_path, self.normal_sprites_sub_dir))\n",
    "        print(self.files[:10])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                self.sprites_path, \n",
    "                self.normal_sprites_sub_dir,\n",
    "                self.files[idx]),\n",
    "        ).convert('RGB')\n",
    "        #image  = image.astype(float)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {\n",
    "            'image': image\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "from torchvision import transforms\n",
    "ds = PokemonDataset(\n",
    "    '../data/external/sprites',\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize((96,96)),\n",
    "        transforms.ToTensor(),\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]['image'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_shape, latent_shape):\n",
    "        super(Encoder, self).__init__()\n",
    "        flattened_size = torch.prod(torch.tensor(input_shape), 0)\n",
    "        self.dense1 = torch.nn.Linear(flattened_size, 256)\n",
    "        self.dense2 = torch.nn.Linear(256, 128)\n",
    "        self.dense3 = torch.nn.Linear(128, 64)\n",
    "        self.dense4 = torch.nn.Linear(64, 32)\n",
    "        self.dense5 = torch.nn.Linear(32, latent_shape)\n",
    "        self.f = nn.Flatten()\n",
    "        #self.dense = torch.nn.Linear()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        x = F.relu(self.dense3(x))\n",
    "        x = F.relu(self.dense4(x))\n",
    "        x = F.relu(self.dense5(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, latent_shape, output_shape):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_flatten_shape = torch.prod(torch.tensor(output_shape), 0).item()\n",
    "        self.output_shape = output_shape\n",
    "        self.dense = torch.nn.Linear(latent_shape, self.output_flatten_shape)\n",
    "\n",
    "        \n",
    "        self.dense1 = nn.Linear(latent_shape, 32)\n",
    "        self.dense2 = nn.Linear(32, 64)\n",
    "        self.dense3 = nn.Linear(64, 128)\n",
    "        self.dense4 = nn.Linear(128, 256)\n",
    "        self.dense5 = nn.Linear(256, self.output_flatten_shape)\n",
    "        #self.dense = torch.nn.Linear()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.dense1(x))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        x = F.relu(self.dense3(x))\n",
    "        x = F.relu(self.dense4(x))\n",
    "        x = F.relu(self.dense5(x))\n",
    "\n",
    "        x = torch.reshape(x, [-1, *self.output_shape])\n",
    "        return x\n",
    "\n",
    "class Auto(torch.nn.Module):\n",
    "    def __init__(self, input_shape, latent_shape):\n",
    "        super(Auto, self).__init__()\n",
    "        self.encoder = Encoder(input_shape, latent_shape)\n",
    "        self.decoder = Decoder(latent_shape, input_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image.fromarray(ds[0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\n",
    "    ds, \n",
    "    batch_size=4, \n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dl))['image'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder((96,96,3), 8)\n",
    "x = encoder(next(iter(dl))['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(8, (3, 96, 96))\n",
    "y = decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = transforms.ToPILImage()(y[0].detach().cpu().data)\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5), (0.5)),\n",
    "])\n",
    "train = torchvision.datasets.FashionMNIST('/tmp', download=True, train=True, transform=transform)\n",
    "trainloader = DataLoader(\n",
    "    train, \n",
    "    batch_size=128, \n",
    "    shuffle=True, \n",
    "    drop_last=True, \n",
    "    num_workers=2,\n",
    "    persistent_workers=True, # makes short epochs start faster\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(tensor):\n",
    "    im = transforms.ToPILImage()(tensor)\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_size = 4\n",
    "ae = Auto((1, 28, 28), latent_size).to(device)\n",
    "\n",
    "tensor = ae(train[0][0].to(device))\n",
    "\n",
    "display(train[0][0])\n",
    "print(tensor.size())\n",
    "display(tensor[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = torch.rand(latent_size) # fix values\n",
    "print(rt.size())\n",
    "ae.decoder(rt.to(device)).cpu().data.size()\n",
    "display(ae.decoder(rt.to(device))[0].cpu().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=0.001)\n",
    "for epoch in range(100):\n",
    "    running_loss = 0\n",
    "    total = 0 # use total as drop_last=True\n",
    "    ae.train()\n",
    "    for image, label in tqdm(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        #print(data[0])\n",
    "        image = image.to(device)\n",
    "        y_pred = ae(image)\n",
    "\n",
    "        loss = criterion(y_pred, image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += image.size(0)\n",
    "    print(f\"loss: {running_loss/total}\")\n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in [0, 100, 1000]:\n",
    "            im = transforms.ToPILImage()(ae(train[idx][0].to(device))[0].cpu().data)\n",
    "            im.show()\n",
    "        rt = torch.rand(latent_size) # fix values\n",
    "        \n",
    "        display(ae.decoder(rt.to(device))[0].cpu().data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.eval()\n",
    "with torch.no_grad():\n",
    "    im = transforms.ToPILImage()(ae(train[1][0].to(device))[0].cpu().data)\n",
    "    im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae(train[1][0].to(device)).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}